{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_L0(language):\n",
    "    return set(language)\n",
    "\n",
    "def calcul_L1(language):\n",
    "    resid = quotient(language,language)\n",
    "    resid.remove(\"\")\n",
    "    return resid\n",
    "\n",
    "\n",
    "def residuel(mot, language):\n",
    "    residuel_set = set()\n",
    "    for word in language:\n",
    "        if word.startswith(mot):\n",
    "            residuel_set.add(word[len(mot):])\n",
    "    return residuel_set\n",
    "\n",
    "def setMoi(set1, set2):\n",
    "    for element in set2:\n",
    "        set1.add(element)\n",
    "\n",
    "def quotient(language1, language2):\n",
    "    quotient_set = set()\n",
    "    for word in language1:\n",
    "        residual_set = residuel(word, language2)\n",
    "        setMoi(quotient_set,residual_set)\n",
    "    return quotient_set\n",
    "\n",
    "def sardinas(language):\n",
    "    l0 = calcul_L0(language)\n",
    "    if(len(l0) == 1):\n",
    "        return True\n",
    "    else:\n",
    "        l1 = calcul_L1(l0)\n",
    "        tab_set = [l0,l1]\n",
    "        return ln(tab_set,2)\n",
    "\n",
    "def ln(tab_set,indice):\n",
    "    lang_princ = tab_set[0]\n",
    "    lang_moins = tab_set[indice-1]\n",
    "    valiny = ln_mitohy(lang_princ,lang_moins)\n",
    "    tab_set.append(valiny)\n",
    "\n",
    "    if len(valiny) == 0:\n",
    "        return True\n",
    "    elif contient_eps(valiny) == True:\n",
    "        return False\n",
    "    elif has_duplicates(tab_set) == True:\n",
    "        return True\n",
    "    else:\n",
    "        return ln(tab_set,indice+1)\n",
    "\n",
    "\n",
    "def ln_mitohy(lang_principal,lang_moins):\n",
    "    resid1 = quotient(lang_principal,lang_moins)\n",
    "    resid2 = quotient(lang_moins,lang_principal)\n",
    "    setMoi(resid1,resid2)\n",
    "    return resid1\n",
    "\n",
    "def contient_eps(set1):\n",
    "    for item in set1:\n",
    "        if item == \"\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_duplicates(tab):\n",
    "    tuple_set = set()\n",
    "    for s in tab:\n",
    "        tuple_set.add(tuple(sorted(s)))\n",
    "    \n",
    "    if len(tuple_set) < len(tab):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longueur_moyenne(language):\n",
    "    taille = len(language)\n",
    "    somme = 0\n",
    "    for el in language:\n",
    "        somme += len(el)\n",
    "    return somme/taille\n",
    "\n",
    "def about_1_0(language):\n",
    "    taille = len(language)\n",
    "    taille_total = 0\n",
    "    nb_0 = 0\n",
    "    nb_1 = 0\n",
    "    for el in language:\n",
    "        taille_total += len(el)\n",
    "        nb_0 += el.count('0')\n",
    "        nb_1 += el.count('1')\n",
    "    moyenne_nb_0 = nb_0/taille\n",
    "    moyenne_nb_1 = nb_1/taille\n",
    "    p_nb_0 = nb_0/taille_total\n",
    "    p_nb_1 = nb_1/taille_total\n",
    "\n",
    "    return moyenne_nb_0,moyenne_nb_1,p_nb_0,p_nb_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_to_matrix(language):\n",
    "    lgm = longueur_moyenne(language)\n",
    "    moyenne_nb_0,moyenne_nb_1,p_nb_0,p_nb_1 = about_1_0(language)\n",
    "    nb_mot = len(language)\n",
    "    return [nb_mot,lgm,moyenne_nb_0,moyenne_nb_1,p_nb_0,p_nb_1]\n",
    "\n",
    "def language_to_matrix2(language,all_binary_words):\n",
    "    matr = language_to_matrix(language)\n",
    "    for words in all_binary_words:\n",
    "        if words in language:\n",
    "            matr.append(1)\n",
    "        else:\n",
    "            matr.append(0)\n",
    "    return matr\n",
    "\n",
    "def donneeToMatrix(donnee,all_binary_words = []):\n",
    "    tab_valeur = []\n",
    "    tab_matrix = []\n",
    "    for d in donnee:\n",
    "        tab_matrix.append(language_to_matrix2(d,all_binary_words))\n",
    "        tab_valeur.append(sardinas(d))\n",
    "    return tab_valeur, tab_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apprentissage(valeur,matrix):\n",
    "    X = np.array(matrix)\n",
    "    y = np.array(valeur)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,random_state = 1)\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    print(a)\n",
    "    return clf\n",
    "\n",
    "def prediction(clf,matrix):\n",
    "    valiny =  clf.predict([matrix])\n",
    "    return valiny[0]\n",
    "\n",
    "def ecrireFichier(clf):\n",
    "        variable_serializee = pickle.dumps(clf)\n",
    "        with open(\"modele.pkl\", \"wb\") as fichier:\n",
    "            fichier.write(variable_serializee)\n",
    "\n",
    "def lireFichier():\n",
    "        with open(\"modele.pkl\", \"rb\") as fichier:\n",
    "            contenu_fichier = fichier.read()\n",
    "        return pickle.loads(contenu_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_all_binary_words(min_length, max_length):\n",
    "    all_words = []\n",
    "    for length in range(min_length, max_length + 1):\n",
    "        all_words.extend([''.join(bits) for bits in itertools.product('01', repeat=length)])\n",
    "    return all_words\n",
    "\n",
    "# Générer toutes les combinaisons possibles de mots binaires de longueur 1 à 7\n",
    "min_length = 1\n",
    "max_length = 7\n",
    "all_binary_words = generate_all_binary_words(min_length, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_languages(words, min_words=2, max_words=10):\n",
    "    all_languages = set()\n",
    "    \n",
    "    for num_words in range(min_words, max_words + 1):\n",
    "        for language in itertools.combinations(words, num_words):\n",
    "            all_languages.add(tuple(sorted(language)))\n",
    "        print(num_words)\n",
    "    \n",
    "    return all_languages\n",
    "        \n",
    "def printtab(tab):\n",
    "    for a in tab:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genererMyLanguage(all_binary_words):\n",
    "    nombre = 3000\n",
    "    li = set()\n",
    "    tab_augment = []\n",
    "\n",
    "    for a in all_binary_words:\n",
    "        tab_augment.append(set([a]))\n",
    "\n",
    "    valiny = ge_more1(tab_augment,all_binary_words)\n",
    "    li.update(random.sample(valiny,nombre))\n",
    "    tab_augment = random.sample(valiny,nombre)\n",
    "    tab_augment = nettoyer(tab_augment)\n",
    "    for i in range (3,11):\n",
    "        val = ge_moreAll(tab_augment,all_binary_words, i)\n",
    "        li.update(random.sample(val,nombre))\n",
    "        tab_augment = random.sample(val,nombre)\n",
    "        tab_augment = nettoyer(tab_augment)\n",
    "    return li\n",
    "\n",
    "def nettoyer(tabtuple):\n",
    "    ma = []\n",
    "    for a in tabtuple:\n",
    "        ma.append(set(a))\n",
    "    return ma\n",
    "\n",
    "def ge_more1(tab_augment,all_binary_words):\n",
    "    valiny = set()\n",
    "    for a in tab_augment:\n",
    "        for el in all_binary_words:\n",
    "            new_tab = set([el]) | a\n",
    "            if(len(new_tab) == 2):\n",
    "                salut = tuple(sorted(new_tab))\n",
    "                valiny.add(salut)\n",
    "    return valiny\n",
    "\n",
    "def ge_moreAll(tab_augment,all_binary_words,indice):\n",
    "    valiny = set()\n",
    "    for a in tab_augment:\n",
    "        for el in all_binary_words:\n",
    "            new_tab = set([el]) | a\n",
    "            if(len(new_tab) == indice):\n",
    "                salut = tuple(sorted(new_tab))\n",
    "                valiny.add(salut)\n",
    "    return valiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9499/3096975828.py:10: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  li.update(random.sample(valiny,nombre))\n",
      "/tmp/ipykernel_9499/3096975828.py:11: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  tab_augment = random.sample(valiny,nombre)\n",
      "/tmp/ipykernel_9499/3096975828.py:15: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  li.update(random.sample(val,nombre))\n",
      "/tmp/ipykernel_9499/3096975828.py:16: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  tab_augment = random.sample(val,nombre)\n"
     ]
    }
   ],
   "source": [
    "mylanguages= genererMyLanguage(all_binary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 3399\n"
     ]
    }
   ],
   "source": [
    "# print(len(mylanguages))\n",
    "code  = []\n",
    "non_code  = []\n",
    "tab = {True:0,False:0}\n",
    "for lan in mylanguages:\n",
    "    sard = sardinas(lan)\n",
    "    if sard:\n",
    "        code.append(lan)\n",
    "    else:\n",
    "        non_code.append(lan)\n",
    "\n",
    "code = random.sample(code,5000)\n",
    "print(len(code), len(non_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def ecrire(fie,language):\n",
    "    with open(fie,'w',newline='') as file:\n",
    "        writer  = csv.writer(file,delimiter=';')\n",
    "        writer.writerows(language)\n",
    "\n",
    "ecrire('mycode.csv',code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  :  32131\n",
      "3  :  2699004\n",
      "4  :  169362501\n",
      "5  :  8468125050\n",
      "6  :  351427189575\n",
      "7  :  12450563287800\n",
      "8  :  384411141510825\n",
      "9  :  10507237867962550\n",
      "10  :  257427327765082475\n"
     ]
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    result = 1\n",
    "    for i in range(2, n + 1):\n",
    "        result *= i\n",
    "    return result\n",
    "\n",
    "def combinations(n, p):\n",
    "    return factorial(n) // (factorial(p) * factorial(n - p))\n",
    "\n",
    "somme = 0\n",
    "for i in range (2,11):\n",
    "    comb = combinations(254,i)\n",
    "    print(i, \" : \", comb)\n",
    "    somme += comb\n",
    "# print(\"total: \",somme)\n",
    "# print(\"comb\", combinations(9,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mycode.csv')\n",
    "df_n = pd.read_csv('mytsy_code.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :].values\n",
    "X_n = df_n.iloc[:, :].values\n",
    "languages = []\n",
    "for d in X:\n",
    "    languages.append(d[0].split(';'))\n",
    "\n",
    "for d in X_n:\n",
    "    languages.append(d[0].split(';'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8213592233009709\n"
     ]
    }
   ],
   "source": [
    "tab_valeur, tab_matrix = donneeToMatrix(languages,all_binary_words)\n",
    "clf = apprentissage(tab_valeur,tab_matrix)\n",
    "ecrireFichier(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "language = [\"000\", \"01\",\"100\", \"010\"]\n",
    "mat_lang = language_to_matrix2(language,all_binary_words)\n",
    "clf = lireFichier()\n",
    "val = prediction(clf,mat_lang)\n",
    "sard = sardinas(language)\n",
    "print(val, sard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
